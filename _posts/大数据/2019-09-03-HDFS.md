---
layout: post
category: 大数据
---

# 1 OverView
> https://support.huawei.com/enterprise/zh/doc/EDOC1000113254?section=j00d
## 1.1 数据格式parquet
# 2 HDFS Site

key | default value| 建议值|说明|
---|---|---|---|---
dfs.block.size | |134217728|128MB|
dfs.replication ||3|副本数|
dfs.datanode.du.reserved ||10737418240【10G】|磁盘预留空间，根据磁盘大小自己定义|
dfs.namenode.handler.count||设置该值的一般原则是将其设置为集群大小的自，然对数乘以20，即20logN，N为集群大小,N表示集群大小 比如50个节点的集群,python -c 'import math ; print int(math.log(50) * 20)',78|*NameNode有一个工作线程池用来处理客户端的远程过程调用及集群守护进程的调用。处理程序数量越多意味着要更大的池来处理来自不同DataNode的并发心跳以及客户端并发的元数据操作。对于大集群或者有大量客户端的集群来说，通常需要增大参数dfs.namenode.handler.count的默认值10|
dfs.datanode.max.xcievers||65536|NameNode同时和DataNode通信的线程数|
dfs.balance.bandwidthPerSec||20485760|默认我1048579（1MB/S）,将其增大为20MB/S|
dfs.datanode.failed.volumes.tolerated|0|2|声明失败前允许多少个硬盘出现故障，服务依然运行|
   ||||
    ||||
     ||||

# 3 Core Site

key | default value| 建议值|说明|
---|---|---|---|---
fs.trash.interval |  |1440分钟=60\*24|检查点被删除后的分钟数|
fs.trash.checkpoint.interval |  |1440分钟=60\*24|垃圾检查点之间的分钟数|
io.file.buffer.size|4096|建议1M|默认4K，以提升 hdfs io |
io.bytes.per.checksum||512|每次进行校验和检查的字节数。一定不能大于io.file.buffer.size.|
 ||||
  ||||
 
 
# 4 balancer
> https://docs.cloudera.com/HDPDocuments/HDP3/HDP-3.1.4/data-storage/content/diskbalancer_commands.html

key | default value| 建议值|说明|
---|---|---|---|---
dfs.disk.balancer.auto.enabled|false|true|是否开始disk balance|
dfs.disk.balancer.auto.cron.expression|0 1 * * 6(每周六一点执行)||周期做自动disk检查|
dfs.disk.balancer.max.disk.throughputInMBperSec|10||执行磁盘数据均衡时可使用的最大磁盘带宽。单位为MB/s,默认值为10|
dfs.disk.balancer.max.disk.errors|5||设置我们能够容忍的在指定的移动过程中出现的最大错误次数，超过此阈值则移动失败|
dfs.disk.balancer.block.tolerance.percent|10||设置磁盘之间进行数据均衡操作时，各个磁盘的数据存储量与完美状态之间的差异阈值。例如，各个磁盘的理想数据存储量为1TB，此参数设置为10。那么，当目标磁盘的数据存储量达到900GB时，就认为该磁盘的存储状态就已经足够好了。取值范围[1-100]。|
dfs.disk.balancer.plan.threshold.percent|10||设置在磁盘数据均衡中可容忍的两磁盘之间的数据密度域值差。如果任意两个磁盘数据密度差值的绝对值超过了此阈值，意味着对应的磁盘应该进行数据均衡。取值范围[1-100]。|
dfs.disk.balancer.top.nodes.number|5||指定集群中排名top N 的需要执行磁盘数据均衡的节点|


![-w499](/assets/img//15750003929177.jpg)

## 4.1 balancer policy
 
```
dfs.datanode.fsdataset.volume.choosing.policy
循环（round-robin）策略将新块均匀分布在可用磁盘上；
而可用空间（ available-space ）策略优先将数据写入具有最大可用空间的磁盘（通过百分比计算的）
```
 
 ![](/assets/img//15749986010461.jpg)


```
Rebalancing Policy
Balancer Default Group 
DataNode
BlockPool
```

![](/assets/img//15749986961024.jpg)

## 4.2 node balancer
## 4.3 disk balancer
 - 配置修改. hdfs->configuration->filters(scope->hdfs(service-wide))->Cluster-wide Advanced Configuration Snippet (Safety Valve) for hdfs-site.xml 
 
```
dfs.disk.balancer.enabled
true
```

![](/assets/img//15750182696865.jpg)

```
修改完之后，重启集群.
hdfs diskbalancer -report
```
![-w1435](/assets/img//15750212654392.jpg)


```
#这个需要在对应的datanode上执行，并且kinit，不然提示没权限
hdfs diskbalancer -plan xx.dn.001.xx.com
#生成plan的json文件
/system/diskbalancer/2019-十一月-29-17-58-57/xxx-cdh-dn-001.com.plan.json
```

![-w1424](/assets/img//15750213073894.jpg)


![-w1439](/assets/img//15750213563540.jpg)



```
hdfs diskbalancer -execute  /system/diskbalancer/2019-十一月-29-17-58-57/xxx-cdh-dn-001.com.plan.json

19/11/29 18:08:17 INFO command.Command: Executing "query plan" command.
```
操作前:
![](/assets/img//15749992494148.jpg)

操作后: